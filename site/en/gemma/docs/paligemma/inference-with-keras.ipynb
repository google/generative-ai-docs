{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3MMAcssHTML"
      },
      "source": [
        "<link rel=\"stylesheet\" href=\"/site-assets/css/gemma.css\">\n",
        "<link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/css2?family=Google+Symbols:opsz,wght,FILL,GRAD@20..48,100..700,0..1,-50..200\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_lX1k54KKrx"
      },
      "source": [
        "Copyright 2024 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Gr4W9nspKGtb"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etcMXWCUJApZ"
      },
      "source": [
        "# Inference with Keras\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5_nIe-8gdJV"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "<td>\n",
        "<a target=\"_blank\" href=\"https://ai.google.dev/gemma/docs/paligemma/inference-with-keras\"><img src=\"https://ai.google.dev/static/site-assets/images/docs/notebook-site-button.png\" height=\"32\" width=\"32\" />View on ai.google.dev</a>\n",
        "</td>\n",
        "<td>\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/google/generative-ai-docs/blob/main/site/en/gemma/docs/paligemma/inference-with-keras.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "</td>\n",
        "<td>\n",
        "<a target=\"_blank\" href=\"https://github.com/google/generative-ai-docs/blob/main/site/en/gemma/docs/paligemma/inference-with-keras.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "</td>\n",
        "</table>\n",
        "\n",
        "When your AI model produces a conclusion or a prediction, it goes through a process called *inference*. This tutorial goes over how to use PaliGemma with Keras to set up a simple model that can infer information about supplied images and answer questions about them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JaII1xxfbfz"
      },
      "source": [
        "## What's in this notebook\n",
        "\n",
        "This notebook uses PaliGemma with Keras and shows you how to:\n",
        "\n",
        "* Install Keras and the required dependencies\n",
        "* Download `PaliGemmaCausalLM`, a pre-trained PaliGemma variant for causal visual language modeling, and use it to create a model\n",
        "* Test the model's ability to infer information about supplied images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bf7AUi02fcPL"
      },
      "source": [
        "## Before you begin\n",
        "\n",
        "Before going through this notebook, you should be familiar with Python code, as well as how large language models (LLMs) are trained. You don't need to be familiar with Keras, but basic knowledge about Keras is helpful when reading through the example code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "380it9kIhzmm"
      },
      "source": [
        "## Setup\n",
        "\n",
        "The following sections explain the preliminary steps for getting a notebook to use a PaliGemma model, including model access, getting an API key, and configuring the notebook runtime."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uN0EpPJh7t5"
      },
      "source": [
        "### Get access to PaliGemma\n",
        "\n",
        "Before using PaliGemma for the first time, you must request access to the model through Kaggle by completing the following steps:\n",
        "\n",
        "1. Log in to [Kaggle](https://www.kaggle.com), or create a new Kaggle account if you don't already have one.\n",
        "1. Go to the [PaliGemma model card](https://www.kaggle.com/models/google/paligemma/) and click **Request Access**.\n",
        "1. Complete the consent form and accept the terms and conditions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uz_pBNrWiDqe"
      },
      "source": [
        "### Configure your API key\n",
        "\n",
        "To use PaliGemma, you must provide your Kaggle username and a Kaggle API key.\n",
        "\n",
        "To generate a Kaggle API key, open your [**Settings** page in Kaggle](https://www.kaggle.com/settings) and click **Create New Token**. This triggers the download of a `kaggle.json` file containing your API credentials.\n",
        "\n",
        "Then, in Colab, select **Secrets** (ðŸ”‘) in the left pane and add your Kaggle username and Kaggle API key. Store your username under the name `KAGGLE_USERNAME` and your API key under the name `KAGGLE_KEY`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUB4JE0Hlxce"
      },
      "source": [
        "### Select the runtime\n",
        "\n",
        "To complete this tutorial, you'll need to have a Colab runtime with sufficient resources to run the PaliGemma model. In this case, you can use a T4 GPU:\n",
        "\n",
        "1. In the upper-right of the Colab window, click the **â–¾ (Additional connection options)** dropdown menu.\n",
        "1. Select **Change runtime type**.\n",
        "1. Under **Hardware accelerator**, select **T4 GPU**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIvv2Yo3lycQ"
      },
      "source": [
        "### Set environment variables\n",
        "\n",
        "Set the environment variables for `KAGGLE_USERNAME`, `KAGGLE_KEY`, and `KERAS_BACKEND`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdgwLyZLQBkP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Set up environmental variables\n",
        "os.environ[\"KAGGLE_USERNAME\"] = userdata.get('KAGGLE_USERNAME')\n",
        "os.environ[\"KAGGLE_KEY\"] = userdata.get('KAGGLE_KEY')\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a3Q4VCLljR9"
      },
      "source": [
        "### Install Keras\n",
        "\n",
        "Run the below cell to install Keras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoYMMytAaMRJ"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q keras-nlp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2Y7BRtRgfvG"
      },
      "source": [
        "### Import dependencies and configure Keras\n",
        "\n",
        "Install the dependencies needed for this notebook and configure Keras' backend. You'll also set Keras to use `bfloat16` so that the framework uses less memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHECpBe6LE7y"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import keras_nlp\n",
        "import numpy as np\n",
        "import PIL\n",
        "import requests\n",
        "import io\n",
        "import matplotlib\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from PIL import Image\n",
        "\n",
        "keras.config.set_floatx(\"bfloat16\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftjt5DiueVkL"
      },
      "source": [
        "## Create your model\n",
        "\n",
        "Now that you've set everything up, you can download the pre-trained model and create some utility methods to help your model generate its responses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-LE2E1uiSpP"
      },
      "source": [
        "### Download the model checkpoint\n",
        "\n",
        "KerasNLP provides implementations of many popular [model architectures](https://keras.io/api/keras_nlp/models/). In this notebook, you'll create a model using `PaliGemmaCausalLM`, an end-to-end PaliGemma model for *causal visual language modeling*. A causal visual language model predicts the next token based on previous tokens.\n",
        "\n",
        "Create the model using the `from_preset` method and print its summary. This process will take about a minute to complete."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abNuIP8D_9At"
      },
      "outputs": [],
      "source": [
        "paligemma = keras_nlp.models.PaliGemmaCausalLM.from_preset(\"pali_gemma_3b_mix_224\")\n",
        "paligemma.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBsWvKEvoGMe"
      },
      "source": [
        "### Create utility methods\n",
        "\n",
        "To help you generate responses from your model, create two utility methods:\n",
        "\n",
        "*   **`crop_and_resize`:** Helper method for `read_img`. This method crops and resizes the image to the passed in size so that the final image is resized without skewing the proportions of the image.\n",
        "*   **`read_img`:** Helper method for `read_img_from_url`. This method is what actually opens the image, resizes it so that it fits in the model's constraints, and puts it into an array that can be interpreted by the model.\n",
        "*   **`read_img_from_url`:** Takes in an image via a valid URL. You need this method to pass the image to the model.\n",
        "\n",
        "You'll use `read_img_from_url` in the next step of this notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6_XQjhpvXiG"
      },
      "outputs": [],
      "source": [
        "def crop_and_resize(image, target_size):\n",
        "    width, height = image.size\n",
        "    source_size = min(image.size)\n",
        "    left = width // 2 - source_size // 2\n",
        "    top = height // 2 - source_size // 2\n",
        "    right, bottom = left + source_size, top + source_size\n",
        "    return image.resize(target_size, box=(left, top, right, bottom))\n",
        "\n",
        "def read_image(url, target_size):\n",
        "    contents = io.BytesIO(requests.get(url).content)\n",
        "    image = PIL.Image.open(contents)\n",
        "    image = crop_and_resize(image, target_size)\n",
        "    image = np.array(image)\n",
        "    # Remove alpha channel if neccessary.\n",
        "    if image.shape[2] == 4:\n",
        "        image = image[:, :, :3]\n",
        "    return image\n",
        "\n",
        "def parse_bbox_and_labels(detokenized_output: str):\n",
        "  matches = re.finditer(\n",
        "      '<loc(?P<y0>\\d\\d\\d\\d)><loc(?P<x0>\\d\\d\\d\\d)><loc(?P<y1>\\d\\d\\d\\d)><loc(?P<x1>\\d\\d\\d\\d)>'\n",
        "      ' (?P<label>.+?)( ;|$)',\n",
        "      detokenized_output,\n",
        "  )\n",
        "  labels, boxes = [], []\n",
        "  fmt = lambda x: float(x) / 1024.0\n",
        "  for m in matches:\n",
        "    d = m.groupdict()\n",
        "    boxes.append([fmt(d['y0']), fmt(d['x0']), fmt(d['y1']), fmt(d['x1'])])\n",
        "    labels.append(d['label'])\n",
        "  return np.array(boxes), np.array(labels)\n",
        "\n",
        "def display_boxes(image, boxes, labels, target_image_size):\n",
        "  h, l = target_size\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.imshow(image)\n",
        "  for i in range(boxes.shape[0]):\n",
        "      y, x, y2, x2 = (boxes[i]*h)\n",
        "      width = x2 - x\n",
        "      height = y2 - y\n",
        "      # Create a Rectangle patch\n",
        "      rect = patches.Rectangle((x, y),\n",
        "                               width,\n",
        "                               height,\n",
        "                               linewidth=1,\n",
        "                               edgecolor='r',\n",
        "                               facecolor='none')\n",
        "      # Add label\n",
        "      plt.text(x, y, labels[i], color='red', fontsize=12)\n",
        "      # Add the patch to the Axes\n",
        "      ax.add_patch(rect)\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "def display_segment_output(image, segment_mask, target_image_size):\n",
        "  # Calculate scaling factors\n",
        "  h, w = target_image_size\n",
        "  x_scale = w / 64\n",
        "  y_scale = h / 64\n",
        "\n",
        "  # Create coordinate grids for the new image\n",
        "  x_coords = np.arange(w)\n",
        "  y_coords = np.arange(h)\n",
        "  x_coords = (x_coords / x_scale).astype(int)\n",
        "  y_coords = (y_coords / y_scale).astype(int)\n",
        "  resized_array = segment_mask[y_coords[:, np.newaxis], x_coords]\n",
        "  # Create a figure and axis\n",
        "  fig, ax = plt.subplots()\n",
        "\n",
        "  # Display the image\n",
        "  ax.imshow(image)\n",
        "\n",
        "  # Overlay the mask with transparency\n",
        "  ax.imshow(resized_array, cmap='jet', alpha=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeVUHA_zP8ZF"
      },
      "source": [
        "## Test your model\n",
        "\n",
        "Now you're ready to give an image and prompt to your model and have it infer the response.\n",
        "\n",
        "Lets look at our test image and read it\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtPeUkGJFvXS"
      },
      "outputs": [],
      "source": [
        "target_size = (224, 224)\n",
        "image_url = 'https://storage.googleapis.com/keras-cv/models/paligemma/cow_beach_1.png'\n",
        "cow_image = read_image(image_url, target_size)\n",
        "matplotlib.pyplot.imshow(cow_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3FE63iMqb2G"
      },
      "source": [
        "Here's a generation call with a single image and prompt. The prompts have to end with a `\\n`.\n",
        "\n",
        "We've supplied you with several example prompts â€” play around with it! Comment and uncomment the prompt variables to change what prompt you supply the model with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQVmhpr6qdNH"
      },
      "outputs": [],
      "source": [
        "prompt = 'answer en where is the cow standing?\\n'\n",
        "# prompt = 'svar no hvor stÃ¥r kuen?'\n",
        "# prompt = 'answer fr quelle couleur est le ciel?'\n",
        "# prompt = 'responda pt qual a cor do animal?'\n",
        "output = paligemma.generate(\n",
        "    inputs={\n",
        "        \"images\": cow_image,\n",
        "        \"prompts\": prompt,\n",
        "    }\n",
        ")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCMqS0vI6S36"
      },
      "source": [
        "Here's a generation call with batched inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZle4sJP6YwB"
      },
      "outputs": [],
      "source": [
        "prompts = [\n",
        "    'answer en where is the cow standing?\\n',\n",
        "    'answer en what color is the cow?\\n',\n",
        "    'describe en\\n',\n",
        "    'detect cow\\n',\n",
        "    'segment cow\\n',\n",
        "]\n",
        "images = [cow_image, cow_image, cow_image, cow_image, cow_image]\n",
        "outputs = paligemma.generate(\n",
        "    inputs={\n",
        "        \"images\": images,\n",
        "        \"prompts\": prompts,\n",
        "    }\n",
        ")\n",
        "for output in outputs:\n",
        "    print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJKvH8to6kb6"
      },
      "source": [
        "We've supplied you with several example prompts â€” play around with it! Comment and uncomment the `prompt` variables to change what prompt you supply the model with."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uqMwZQY31mu"
      },
      "source": [
        "### Other styles of prompts\n",
        "\n",
        "You may have noticed in the previous step that the provided examples are in several different languages. PaliGemma supports language recognition for 34 different languages. You can find the list of supported languages on [GitHub](https://github.com/google/crossmodal-3600/blob/main/web-data/README.md).\n",
        "\n",
        "PaliGemma can handle several other prompt styles:\n",
        "\n",
        "*   **`\"cap {lang}\\n\"`:** Very raw short caption (from WebLI-alt)\n",
        "*   **`\"caption {lang}\\n\"`:** Nice, COCO-like short captions\n",
        "*   **`\"describe {lang}\\n\"`:** Somewhat longer, more descriptive captions\n",
        "*   **`\"ocr\"`:** Optical character recognition\n",
        "*   **`\"answer en {question}\\n\"`:** Question answering about the image contents\n",
        "*   **`\"question {lang} {answer}\\n\"`:** Question generation for a given answer\n",
        "*   **`\"detect {object} ; {object}\\n\"`:** Count objects in a scene and return the bounding boxes for the objects\n",
        "*   **`\"segment {object}\\n\"`:** Do image segmentation of the object in the scene\n",
        "\n",
        "Try them out!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUgPcUGDpsdx"
      },
      "source": [
        "### Parse detect output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "noRIZtqnpzNn"
      },
      "outputs": [],
      "source": [
        "prompt = 'detect cow\\n'\n",
        "output = paligemma.generate(\n",
        "    inputs={\n",
        "        \"images\": cow_image,\n",
        "        \"prompts\": prompt,\n",
        "    }\n",
        ")\n",
        "boxes, labels = parse_bbox_and_labels(output)\n",
        "display_boxes(cow_image, boxes, labels, target_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUwNeaV0sGau"
      },
      "source": [
        "### Parse segment output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tuxyIMAvsM4B"
      },
      "outputs": [],
      "source": [
        "# @title  Fetch big_vision code and install dependencies.</font></center>\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# TPUs with\n",
        "if \"COLAB_TPU_ADDR\" in os.environ:\n",
        "  raise \"It seems you are using Colab with remote TPUs which is not supported.\"\n",
        "\n",
        "# Fetch big_vision repository if python doesn't know about it and install\n",
        "# dependencies needed for this notebook.\n",
        "if not os.path.exists(\"big_vision_repo\"):\n",
        "  !git clone --quiet --branch=main --depth=1 \\\n",
        "     https://github.com/google-research/big_vision big_vision_repo\n",
        "\n",
        "# Append big_vision code to python import path\n",
        "if \"big_vision_repo\" not in sys.path:\n",
        "  sys.path.append(\"big_vision_repo\")\n",
        "\n",
        "\n",
        "# Install missing dependencies. Assume jax~=0.4.25 with GPU available.\n",
        "!pip3 install -q \"overrides\" \"ml_collections\" \"einops~=0.7\" \"sentencepiece\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18S4uHWqutps"
      },
      "source": [
        "Let's take a look at another example image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Al6NedG4uNhp"
      },
      "outputs": [],
      "source": [
        "cat = read_image('https://big-vision-paligemma.hf.space/file=examples/barsik.jpg', target_size)\n",
        "matplotlib.pyplot.imshow(cat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRJYikeXu1As"
      },
      "source": [
        "Here is a function to help parse the segment output from PaliGemma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ryCMfGVuxyd"
      },
      "outputs": [],
      "source": [
        "import  big_vision.evaluators.proj.paligemma.transfers.segmentation as segeval\n",
        "reconstruct_masks = segeval.get_reconstruct_masks('oi')\n",
        "def parse_segments(detokenized_output: str) -> tuple[np.ndarray, np.ndarray]:\n",
        "  matches = re.finditer(\n",
        "      '<loc(?P<y0>\\d\\d\\d\\d)><loc(?P<x0>\\d\\d\\d\\d)><loc(?P<y1>\\d\\d\\d\\d)><loc(?P<x1>\\d\\d\\d\\d)>'\n",
        "      + ''.join(f'<seg(?P<s{i}>\\d\\d\\d)>' for i in range(16)),\n",
        "      detokenized_output,\n",
        "  )\n",
        "  boxes, segs = [], []\n",
        "  fmt_box = lambda x: float(x) / 1024.0\n",
        "  for m in matches:\n",
        "    d = m.groupdict()\n",
        "    boxes.append([fmt_box(d['y0']), fmt_box(d['x0']), fmt_box(d['y1']), fmt_box(d['x1'])])\n",
        "    segs.append([int(d[f's{i}']) for i in range(16)])\n",
        "  return np.array(boxes), np.array(reconstruct_masks(np.array(segs)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QITD66qJvCTO"
      },
      "source": [
        "Query PaliGemma to segment the cat in the image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fB7to-J4u5zY"
      },
      "outputs": [],
      "source": [
        "prompt = 'segment cat\\n'\n",
        "output = paligemma.generate(\n",
        "    inputs={\n",
        "        \"images\": cat,\n",
        "        \"prompts\": prompt,\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZeu6-bovFvz"
      },
      "source": [
        "Visualize the generated mask from PaliGemma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GcjOvoPbvAI-"
      },
      "outputs": [],
      "source": [
        "_, seg_output = parse_segments(output)\n",
        "display_segment_output(cat, seg_output[0], target_size)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "inference-with-keras.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
