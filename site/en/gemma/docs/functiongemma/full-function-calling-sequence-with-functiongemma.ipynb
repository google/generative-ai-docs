{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4adf29ec",
      "metadata": {
        "id": "RyMzM8eYxPxi"
      },
      "source": [
        "Project: /gemma/_project.yaml\n",
        "Book: /gemma/_book.yaml\n",
        "\n",
        "<link rel=\"stylesheet\" href=\"/site-assets/css/style.css\">\n",
        "<link rel=\"stylesheet\" href=\"/site-assets/css/gemma.css\">\n",
        "<link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/css2?family=Google+Symbols:opsz,wght,FILL,GRAD@20..48,100..700,0..1,-50..200\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54635d49",
      "metadata": {
        "id": "DzkEdu7YxUOM"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://ai.google.dev/gemma/docs/functiongemma/full-function-calling-sequence-with-functiongemma\"><img src=\"https://ai.google.dev/static/site-assets/images/docs/notebook-site-button.png\" height=\"32\" width=\"32\" />View on ai.google.dev</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/google/generative-ai-docs/blob/main/site/en/gemma/docs/functiongemma/full-function-calling-sequence-with-functiongemma.ipynb\"\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/google/generative-ai-docs/blob/main/site/en/gemma/docs/functiongemma/full-function-calling-sequence-with-functiongemma.ipynb\"><img src=\"https://www.kaggle.com/static/images/logos/kaggle-logo-transparent-300.png\" height=\"32\" width=\"70\"/>Run in Kaggle</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://console.cloud.google.com/vertex-ai/colab/import/https%3A%2F%2Fraw.githubusercontent.com%2Fgoogle%2Fgenerative-ai-docs%2Fmain%2Fsite%2Fen%2Fgemma%2Fdocs%2Ffunctiongemma%2Ffull-function-calling-sequence-with-functiongemma.ipynb\"><img src=\"https://ai.google.dev/images/cloud-icon.svg\" width=\"40\" />Open in Vertex AI</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/google/generative-ai-docs/blob/main/site/en/gemma/docs/functiongemma/full-function-calling-sequence-with-functiongemma.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "10fcc388",
      "metadata": {
        "id": "sK0ai97dFKEK"
      },
      "source": [
        "# Full function calling sequence with FunctionGemma"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "13355276",
      "metadata": {
        "id": "vIvDYHMdE0Nu"
      },
      "source": [
        "FunctionGemma is a specialized version of the Gemma 3 270M model, trained specifically for function calling improvements. It has the same architecture as Gemma, but uses a different chat format and tokenizer."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "19498bba",
      "metadata": {
        "id": "4CExFIpDKXj7"
      },
      "source": [
        "This guide shows the complete workflow for using FunctionGemma within the Hugging Face ecosystem. It covers the essential setup steps, including installing necessary Python packages like `torch` and `transformers`, and loading the model via the Hugging Face Hub. The core of the tutorial demonstrates a three-stage cycle for connecting the model to external tools: the **Model's Turn** to generate function call objects, the **Developer's Turn** to parse and execute code (such as a weather API), and the **Final Response** where the model uses the tool's output to answer the user."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "810c3e55",
      "metadata": {
        "id": "aDbWVKY9E6_L"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Before starting this tutorial, complete the following steps:\n",
        "\n",
        "* Get access to FunctionGemma by logging into [Hugging Face](https://huggingface.co/google/functiongemma-270m-it) and selecting **Acknowledge license** for a FunctionGemma model.\n",
        "* Generate a Hugging Face [Access Token](https://huggingface.co/docs/hub/en/security-tokens#how-to-manage-user-access-token) and add it to your Colab environment.\n",
        "\n",
        "This notebook will run on either CPU or GPU."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "3fa3de6d",
      "metadata": {
        "id": "foVLY_PSE80d"
      },
      "source": [
        "## Install Python packages\n",
        "\n",
        "Install the Hugging Face libraries required for running the FunctionGemma model and making requests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54361aff",
      "metadata": {
        "id": "TepIbq8hE-2m"
      },
      "outputs": [],
      "source": [
        "# Install PyTorch & other libraries\n",
        "!pip install torch\n",
        "\n",
        "# Install the transformers library\n",
        "!pip install transformers"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "af89da13",
      "metadata": {
        "id": "4VcVqoP7FCd1"
      },
      "source": [
        "After you have accepted the license, you need a valid Hugging Face Token to access the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "086d2a28",
      "metadata": {
        "id": "9Rt_YSIEFDhl"
      },
      "outputs": [],
      "source": [
        "# Login into Hugging Face Hub\n",
        "from huggingface_hub import login\n",
        "login()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "7c6dd475",
      "metadata": {
        "id": "u4r-lcWnFGVa"
      },
      "source": [
        "## Load Model\n",
        "\n",
        "Use the `torch` and `transformers` libraries to create an instance of a `processor` and `model` using the `AutoProcessor` and `AutoModelForCausalLM` classes as shown in the following code example:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b827117",
      "metadata": {
        "id": "ei-6KVNDGVrv"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoProcessor, AutoModelForCausalLM\n",
        "\n",
        "GEMMA_MODEL_ID = \"google/functiongemma-270m-it\"\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(GEMMA_MODEL_ID, device_map=\"auto\")\n",
        "model = AutoModelForCausalLM.from_pretrained(GEMMA_MODEL_ID, dtype=\"auto\", device_map=\"auto\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "43fab412",
      "metadata": {
        "id": "w0ON4iDKIsJ1"
      },
      "source": [
        "## Example Use Cases\n",
        "\n",
        "Function calling connects the generative capabilities of Gemma and the external data and services. Here are some common applications:\n",
        "\n",
        "* **Answering Questions with Real-Time Data:** Use a search engine or weather API to answer questions like \"What's the weather in Tokyo?\" or \"Who won the latest F1 race?\"\n",
        "* **Controlling External Systems:** Connect Gemma to other applications to perform actions, such as sending emails (\"Send a reminder to the team about the meeting\"), managing a calendar, or controlling smart home devices.\n",
        "* **Creating Complex Workflows**: Chain multiple tool calls together to accomplish multi-step tasks, like planning a trip by finding flights, booking a hotel, and creating a calendar event."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "ed7c5023",
      "metadata": {
        "id": "uKq2WuxwLvxz"
      },
      "source": [
        "## Using Tools\n",
        "\n",
        "The core of function calling involves a four-step process:\n",
        "\n",
        "1.  **Define Tools**: Create the functions your model can use, specifying arguments and descriptions (e.g., a weather lookup function).\n",
        "2.  **Model's Turn**: FunctionGemma receives the user's prompt and a list of available tools. It generates a special object indicating which function to call and with what arguments instead of a plain text response.\n",
        "3.  **Developer's Turn**: Your code receives this object, executes the specified function with the provided arguments, and formats the result to be sent back to the model.\n",
        "4.  **Final Response**: FunctionGemma uses the function's output to generate a final, user-facing response.\n",
        "\n",
        "Let's simulate this process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b326ac4",
      "metadata": {
        "id": "H0c-BlgCMCHf"
      },
      "outputs": [],
      "source": [
        "# Define a function that our model can use.\n",
        "def get_current_weather(location: str, unit: str = \"celsius\"):\n",
        "    \"\"\"\n",
        "    Gets the current weather in a given location.\n",
        "\n",
        "    Args:\n",
        "        location: The city and state, e.g. \"San Francisco, CA\" or \"Tokyo, JP\"\n",
        "        unit: The unit to return the temperature in. (choices: [\"celsius\", \"fahrenheit\"])\n",
        "\n",
        "    Returns:\n",
        "        temperature: The current temperature in the given location\n",
        "        weather: The current weather in the given location\n",
        "    \"\"\"\n",
        "    return {\"temperature\": 15, \"weather\": \"sunny\"}\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "f1e1004c",
      "metadata": {
        "id": "073sKeCROy0Y"
      },
      "source": [
        "### Model's Turn\n",
        "\n",
        "We have the user's prompt `\"Hey, what's the weather in Tokyo right now?\"`, and the tool `[get_current_weather]`. FunctionGemma generates a function call object as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76b3d5dd",
      "metadata": {
        "id": "LdSVXUlDMqrQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt: Hey, what's the weather in Tokyo right now?\n",
            "Tools: [<function get_current_weather at 0x79b7e0f52e80>]\n",
            "Output: <start_function_call>call:get_current_weather{location:<escape>Tokyo, Japan<escape>}<end_function_call>\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Hey, what's the weather in Tokyo right now?\"\n",
        "tools = [get_current_weather]\n",
        "\n",
        "message = [\n",
        "        # ESSENTIAL SYSTEM PROMPT:\n",
        "        # This line activates the model's function calling logic.\n",
        "        {\"role\": \"developer\", \"content\": \"You are a model that can do function calling with the following functions\"},\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "]\n",
        "\n",
        "inputs = processor.apply_chat_template(message, tools=tools, add_generation_prompt=True, return_dict=True, return_tensors=\"pt\")\n",
        "output = processor.decode(inputs[\"input_ids\"][0], skip_special_tokens=False)\n",
        "\n",
        "out = model.generate(**inputs.to(model.device), pad_token_id=processor.eos_token_id, max_new_tokens=128)\n",
        "generated_tokens = out[0][len(inputs[\"input_ids\"][0]):]\n",
        "output = processor.decode(generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "print(f\"Prompt: {prompt}\")\n",
        "print(f\"Tools: {tools}\")\n",
        "print(f\"Output: {output}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "78c3ae17",
      "metadata": {
        "id": "1wcX4GCtNfYF"
      },
      "source": [
        "> NOTE: To ensure FunctionGemma correctly interprets the available tools and generates a structured call instead of plain text, the **developer** message is essential. This specific system prompt instructs the model that it has permission and capability to perform function calling.\n",
        "\n",
        "```python\n",
        "message = [\n",
        "        # ESSENTIAL SYSTEM PROMPT:\n",
        "        # This line activates the model's function calling logic.\n",
        "        {\"role\": \"developer\", \"content\": \"You are a model that can do function calling with the following functions\"},\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "]\n",
        "```"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "1c246cf2",
      "metadata": {
        "id": "LCR4CN1BPDOT"
      },
      "source": [
        "### Developer's Turn\n",
        "\n",
        "Your application should parse the model's response to extract function name and argments, and append function call result with the `tool` role.\n",
        "\n",
        "> NOTE: Always validate function names and arguments before execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e402c279",
      "metadata": {
        "id": "xSpMdK56Bu4n"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'role': 'assistant', 'tool_calls': [{'type': 'function', 'function': {'name': 'get_current_weather', 'arguments': {'location': 'Tokyo, Japan'}}}]}\n",
            "{'role': 'tool', 'content': [{'name': 'get_current_weather', 'response': {'temperature': 15, 'weather': 'sunny'}}]}\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def extract_tool_calls(text):\n",
        "    def cast(v):\n",
        "        try: return int(v)\n",
        "        except:\n",
        "            try: return float(v)\n",
        "            except: return {'true': True, 'false': False}.get(v.lower(), v.strip(\"'\\\"\"))\n",
        "\n",
        "    return [{\n",
        "        \"name\": name,\n",
        "        \"arguments\": {\n",
        "            k: cast((v1 or v2).strip())\n",
        "            for k, v1, v2 in re.findall(r\"(\\w+):(?:<escape>(.*?)<escape>|([^,}]*))\", args)\n",
        "        }\n",
        "    } for name, args in re.findall(r\"<start_function_call>call:(\\w+)\\{(.*?)\\}<end_function_call>\", text, re.DOTALL)]\n",
        "\n",
        "calls = extract_tool_calls(output)\n",
        "if calls:\n",
        "    message.append({\n",
        "        \"role\": \"assistant\",\n",
        "        \"tool_calls\": [{\"type\": \"function\", \"function\": call} for call in calls]\n",
        "    })\n",
        "    print(message[-1])\n",
        "\n",
        "    # Call the function and get the result\n",
        "    #####################################\n",
        "    # WARNING: This is a demonstration. #\n",
        "    #####################################\n",
        "    # Using globals() to call functions dynamically can be dangerous in\n",
        "    # production. In a real application, you should implement a secure way to\n",
        "    # map function names to actual function calls, such as a predefined\n",
        "    # dictionary of allowed tools and their implementations.\n",
        "    results = [\n",
        "        {\"name\": c['name'], \"response\": globals()[c['name']](**c['arguments'])}\n",
        "        for c in calls\n",
        "    ]\n",
        "\n",
        "    message.append({\n",
        "        \"role\": \"tool\",\n",
        "        \"content\": results\n",
        "    })\n",
        "    print(message[-1])\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "059f8f09",
      "metadata": {
        "id": "Hgn011cAVj5r"
      },
      "source": [
        "> Note: For optimal results, append the tool execution result to your message history using the specific format below. This ensures the chat template correctly generates the required token structure (e.g., `response:get_current_weather{temperature:15,weather:<escape>sunny<escape>}`).\n",
        "\n",
        "```python\n",
        "message.append({\n",
        "    \"role\": \"tool\",\n",
        "    \"content\": {\n",
        "        \"name\": function_name,\n",
        "        \"response\": function_response\n",
        "    }\n",
        "})\n",
        "```\n",
        "\n",
        "In case of multiple independent requests:\n",
        "\n",
        "```python\n",
        "message.append({\n",
        "    \"role\": \"tool\",\n",
        "    \"content\": [\n",
        "        {\n",
        "            \"name\": function_name_1,\n",
        "            \"response\": function_response_1\n",
        "        },\n",
        "        {\n",
        "            \"name\": function_name_2,\n",
        "            \"response\": function_response_2\n",
        "        }\n",
        "    ]\n",
        "})\n",
        "```\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "924d401d",
      "metadata": {
        "id": "TNar4GXbVECf"
      },
      "source": [
        "### Final Response\n",
        "\n",
        "Finally, FunctionGemma reads the tool response and reply to the user."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be65d1d7",
      "metadata": {
        "id": "yxJxMEStVDmx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output: The current weather in Tokyo is sunny with a temperature of 15 degrees Celsius.\n"
          ]
        }
      ],
      "source": [
        "inputs = processor.apply_chat_template(message, tools=tools, add_generation_prompt=True, return_dict=True, return_tensors=\"pt\")\n",
        "out = model.generate(**inputs.to(model.device), pad_token_id=processor.eos_token_id, max_new_tokens=128)\n",
        "generated_tokens = out[0][len(inputs[\"input_ids\"][0]):]\n",
        "output = processor.decode(generated_tokens, skip_special_tokens=True)\n",
        "print(f\"Output: {output}\")\n",
        "message.append({\"role\": \"assistant\", \"content\": output})"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "fca2108d",
      "metadata": {
        "id": "yqqLBasqw1Yi"
      },
      "source": [
        "You can see the full chat history below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c34ec71e",
      "metadata": {
        "id": "hG5t3-zecQGl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'role': 'developer', 'content': 'You are a model that can do function calling with the following functions'}\n",
            "{'role': 'user', 'content': \"Hey, what's the weather in Tokyo right now?\"}\n",
            "{'role': 'assistant', 'tool_calls': [{'type': 'function', 'function': {'name': 'get_current_weather', 'arguments': {'location': 'Tokyo, Japan'}}}]}\n",
            "{'role': 'tool', 'content': [{'name': 'get_current_weather', 'response': {'temperature': 15, 'weather': 'sunny'}}]}\n",
            "{'role': 'assistant', 'content': 'The current weather in Tokyo is sunny with a temperature of 15 degrees Celsius.'}\n",
            "--------------------------------------------------------------------------------\n",
            "Output: <bos><start_of_turn>developer\n",
            "You are a model that can do function calling with the following functions<start_function_declaration>declaration:get_current_weather{description:<escape>Gets the current weather in a given location.<escape>,parameters:{properties:{location:{description:<escape>The city and state, e.g. \"San Francisco, CA\" or \"Tokyo, JP\"<escape>,type:<escape>STRING<escape>},unit:{description:<escape>The unit to return the temperature in.<escape>,enum:[<escape>celsius<escape>,<escape>fahrenheit<escape>],type:<escape>STRING<escape>}},required:[<escape>location<escape>],type:<escape>OBJECT<escape>}}<end_function_declaration><end_of_turn>\n",
            "<start_of_turn>user\n",
            "Hey, what's the weather in Tokyo right now?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "<start_function_call>call:get_current_weather{location:<escape>Tokyo, Japan<escape>}<end_function_call><start_function_response>response:get_current_weather{temperature:15,weather:<escape>sunny<escape>}<end_function_response>The current weather in Tokyo is sunny with a temperature of 15 degrees Celsius.<end_of_turn>\n"
          ]
        }
      ],
      "source": [
        "# full history\n",
        "for item in message:\n",
        "  print(item)\n",
        "\n",
        "print(\"-\"*80)\n",
        "output = processor.decode(out[0], skip_special_tokens=False)\n",
        "print(f\"Output: {output}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "7521496f",
      "metadata": {
        "id": "j-al9BVPFN6Q"
      },
      "source": [
        "## Summary and next steps\n",
        "\n",
        "You have established how to build an application that can calls functions with FunctionGemma. The workflow is established through a four-stage cycle:\n",
        "\n",
        "1.  **Define Tools**: Create the functions your model can use, specifying arguments and descriptions (e.g., a weather lookup function).\n",
        "2.  **Model's Turn**: The model receives the user's prompt and a list of available tools, returning a structured function call object instead of plain text.\n",
        "3.  **Developer's Turn**: The developer parses this output using regular expressions to extract function names and arguments, executes the actual Python code, and appends the results to the chat history using the specific tool role.\n",
        "4. **Final Response**: The model processes the tool's execution result to generate a final, natural language answer for the user.\n",
        "\n",
        "Check out the following documentation for further reading.\n",
        "\n",
        "- [Fine-tuning with FunctionGemma](https://ai.google.dev/gemma/docs/functiongemma/finetuning-with-functiongemma)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "full-function-calling-sequence-with-functiongemma.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
